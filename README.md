# ğŸ” Keyword URL Crawler

**Keyword URL Crawler** is a two-part Python tool designed to help you **scrape and filter URLs based on a specific keyword** and **validate those URLs** to ensure they are accessible (status code 200). Perfect for web scraping, SEO analysis, or digital research.

---

## ğŸ§  Features

- âœ… Recursive crawling of any website
- ğŸ” Extracts and filters URLs containing a specific keyword
- ğŸŒ Resolves relative URLs to full URLs
- ğŸš« Skips duplicate URLs with visited URL tracking
- âš™ï¸ Validates URLs to check if they're alive (status 200)
- ğŸ“ Saves working URLs to a text file

---

## ğŸ“‚ Project Structure

keyword-url-crawler/
â”‚
â”œâ”€â”€ keyword_crawler.py # Crawls and extracts keyword-matching URLs
â”œâ”€â”€ url_validator.py # Validates URLs and saves working ones
â”œâ”€â”€ filtered_urls.txt # Output file for valid URLs
â””â”€â”€ README.md # Project documentation

---

## ğŸ’¡ How It Works

### 1. **Crawl URLs (keyword-based)**  
Use `keyword_crawler.py` to recursively find all `<a>` tag links starting from a base URL. It will print only those URLs that contain the keyword you specify.

### 2. **Validate URLs**  
Use `url_validator.py` to check which of the collected URLs return a status code of 200 (working links). These are saved to `filtered_urls.txt`.

---

## ğŸš€ Getting Started

### ğŸ“‹ Requirements

- Python 3.x
- Install dependencies:
  ```bash
  pip install requests beautifulsoup4

## Usage

### âœ… Step 1: Run the Keyword Crawler

 -python keyword_crawler.py

--You will be prompted to enter the base URL and keyword:

-enter a url you would like to scrape: https://example.com
-enter the keyword you want in url: blog

-It will output all URLs containing the keyword blog.

-To save the results to a file:
--python keyword_crawler.py > raw_urls.txt

### ğŸ” Step 2: Validate the URLs
-Pass the saved URLs to the validator using:
--cat raw_urls.txt | python url_validator.py
-Valid URLs will be saved to:
--filtered_urls.txt

ğŸ§ª Example Workflow
--python keyword_crawler.py > raw_urls.txt
--cat raw_urls.txt | python url_validator.py

ğŸ“Œ Notes
â—‡ This crawler is not JavaScript-aware. It only extracts static HTML links.

â—‡ Avoid crawling large or protected websites excessively to prevent being blocked.

â—‡ For ethical use only: Do not crawl private or rate-limited websites without permission.
---

ğŸ” Disclaimer
This tool is intended for educational and ethical use only. Ensure you have permission before crawling any external site.

ğŸ™‹â€â™€ï¸ Author
Samiksha
 Cybersecurity & Forensics Student | Developer | Ethical Hacking Enthusiast

ğŸ¤ Contributing
Pull requests, ideas, and suggestions are welcome!

---
